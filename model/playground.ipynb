{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.io.image import read_image\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn, FasterRCNN_ResNet50_FPN_Weights\n",
    "from torchvision.models import ResNet50_Weights\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = FasterRCNN_ResNet50_FPN_Weights.DEFAULT\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = fasterrcnn_resnet50_fpn(weights=weights, num_classes=len(weights.meta[\"categories\"]), weights_backbone=ResNet50_Weights.DEFAULT).to(device)\n",
    "model.eval()\n",
    "preprocess = weights.transforms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'boxes': tensor([[692.9408, 286.2021, 935.4335, 815.8149],\n",
       "         [313.8521, 456.7144, 469.8397, 810.5626],\n",
       "         [534.9616, 447.1707, 604.5193, 561.5443],\n",
       "         [646.1714, 398.4750, 714.6689, 717.5959],\n",
       "         [583.5797, 393.5256, 660.8668, 729.6469],\n",
       "         [686.7581, 728.0333, 742.6044, 823.1209],\n",
       "         [313.1302, 494.9341, 391.5665, 631.1080]], grad_fn=<IndexBackward0>),\n",
       " 'labels': tensor([ 1,  1, 31,  1,  1, 31,  1]),\n",
       " 'scores': tensor([0.9987, 0.9859, 0.9841, 0.9770, 0.9753, 0.9714, 0.8051],\n",
       "        grad_fn=<IndexBackward0>)}"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = Image.open(\"test.jpeg\").convert('RGB')\n",
    "img_tensor_i = transforms.Compose([\n",
    "    transforms.PILToTensor()\n",
    "])(img)\n",
    "img_tensor_f = transforms.ToTensor()(img)\n",
    "prep_img = preprocess(img_tensor_f).unsqueeze_(0)\n",
    "prediction = model(prep_img)[0]\n",
    "threshold = 0.75\n",
    "prediction['boxes'] = prediction['boxes'][prediction['scores'] > threshold]\n",
    "prediction['labels'] = prediction['labels'][prediction['scores'] > threshold]\n",
    "prediction['scores'] = prediction['scores'][prediction['scores'] > threshold]\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['person - [692.9407958984375, 286.20208740234375, 935.4334716796875, 815.81494140625] - 100.00%',\n",
       " 'person - [313.85205078125, 456.7144470214844, 469.8396911621094, 810.5626220703125] - 99.00%',\n",
       " 'handbag - [534.9616088867188, 447.1706848144531, 604.519287109375, 561.5443115234375] - 98.00%',\n",
       " 'person - [646.17138671875, 398.4749755859375, 714.6688842773438, 717.595947265625] - 98.00%',\n",
       " 'person - [583.5797119140625, 393.52557373046875, 660.8667602539062, 729.6469116210938] - 98.00%',\n",
       " 'handbag - [686.758056640625, 728.0333251953125, 742.6043701171875, 823.1209106445312] - 97.00%',\n",
       " 'person - [313.13018798828125, 494.93414306640625, 391.56646728515625, 631.1080322265625] - 81.00%']"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = [\"{}: {:.2f}%\".format(weights.meta[\"categories\"][prediction[\"labels\"][i]], round(float(prediction['scores'][i]) * 100, 0)) for i in range(len(prediction[\"labels\"]))]\n",
    "box = draw_bounding_boxes(image = img_tensor_i, boxes=prediction[\"boxes\"],\n",
    "                          labels=labels,\n",
    "                          colors=(0, 255, 42),\n",
    "                          width=2, \n",
    "                          font_size=17,\n",
    "                          font='Arial')\n",
    "\n",
    "im = to_pil_image(box.detach())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "397704579725e15f5c7cb49fe5f0341eb7531c82d19f2c29d197e8b64ab5776b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
